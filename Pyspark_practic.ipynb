{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GZBgvirloxDyRoSsYjJ9OrG1J-2MCjl3",
      "authorship_tag": "ABX9TyM2VVmBEgTtNLoxCeN83P88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushabhbhagat08/Pyspark/blob/main/Pyspark_practic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learn Pyspark"
      ],
      "metadata": {
        "id": "-_bMx9zokpkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Installation Of pyspark Library"
      ],
      "metadata": {
        "id": "-5hCZVYwkzxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYzqrfkjxKbX",
        "outputId": "f7917fd3-b9b1-42d8-9b0b-13312cfc5660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=071a6b4691c08216c149c6c9f9b36b4340efc9224fdac8e2e586a666142a0b94\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "#installation pyspark library\n",
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pyspark freamwork\n",
        "import pyspark"
      ],
      "metadata": {
        "id": "SsuOGZjTbkZ6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a pyspark sessions"
      ],
      "metadata": {
        "id": "sxpp9GoDpffU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sparksession from pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "H6p2W8vmorGt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build session\n",
        "spark=SparkSession.builder.appName('pyspark_practise').getOrCreate()"
      ],
      "metadata": {
        "id": "I0NhBEOpp2-y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see the session\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "xaIGXpUxqdN6",
        "outputId": "d6e81984-bbfe-415c-928e-bf18363c16e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ea978f08eb0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8b7e8d5734d4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark_practise</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are create a pyspark seeeion our app name is Pyspark_practise. Here we execute a local machine so, there is only one cluster but when we execute in a cloud there are number of clusters. The spark version we use that is V3.5.0."
      ],
      "metadata": {
        "id": "UGUfvr-bqp2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Reading the Dataset"
      ],
      "metadata": {
        "id": "fxPuqbGwrw68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qc6-T1ueUvb",
        "outputId": "65703e48-13ae-4294-cab7-3bb310037d4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file from drive\n",
        "df_pyspark=spark.read.csv(\"/content/drive/MyDrive/Pyspark/Pyspark_demo.csv\")\n",
        "# df_pyspark1=spark.read.csv('/content/drive/MyDrive/Pyspark/Pyspark_demo.csv')"
      ],
      "metadata": {
        "id": "qSOq_4_Eqg6K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see tthe dataset\n",
        "df_pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILzFVGWVr-aN",
        "outputId": "417ae255-592f-4016-999e-001f666135db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Pyspark Dataframe"
      ],
      "metadata": {
        "id": "h-wF338Sge9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overview of the dataset\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGx56IsrsG8r",
        "outputId": "143e51a0-b750-4517-b24b-cd25c363dda0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|             _c0|          _c1|       _c2|   _c3|\n",
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in this data fream show the two columns i.e. C0 and C1. But its show the C0 and C1 as a header so, we modified that header lets see the new command."
      ],
      "metadata": {
        "id": "RzwDXiPYtUQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed the header of the dataset\n",
        "spark.read.option('header','true').csv('/content/drive/MyDrive/Pyspark/Pyspark_demo.csv').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBLrAWDgsJxo",
        "outputId": "10dbf9c4-6a49-41a3-a2c4-89b0d20ed491"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark=spark.read.option('header','true').csv('/content/drive/MyDrive/Pyspark/Pyspark_demo.csv')"
      ],
      "metadata": {
        "id": "O7-T6bZVuZt5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Types of dataFrame"
      ],
      "metadata": {
        "id": "Gwx29agJuqsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type of the dataframe\n",
        "type(df_pyspark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJaHwb6eumHK",
        "outputId": "46b7bd85-224a-4eb1-e2f3-c06f3f7fa1bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### show the top rows in dataframe"
      ],
      "metadata": {
        "id": "ocorGEY4vI_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 3 rows in dataframe\n",
        "df_pyspark.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBzIGrnTu5GK",
        "outputId": "ef8ef305-6444-45de-db36-04ab602153ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Employee name='Mayur Kalamkar', Employee Age ='25', Experience='8', Salary='70000'),\n",
              " Row(Employee name='Sharad Dhole ', Employee Age ='26', Experience='4', Salary='40000'),\n",
              " Row(Employee name='Rushabh Bhagat', Employee Age ='26', Experience='3', Salary='30000')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Checking Datatype and columns"
      ],
      "metadata": {
        "id": "b2G4Afu0gzqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about dataset\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjvvItU1vSj6",
        "outputId": "5fe8a498-2020-420d-b397-36083581fed7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Employee name: string (nullable = true)\n",
            " |-- Employee Age : string (nullable = true)\n",
            " |-- Experience: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It gives the information about your dataset.But now here to see the Age and Experience columns is integer datatype but its show string. so, there is a option in csv i.e. inferscheme this by default false so it show the all values are string datatype."
      ],
      "metadata": {
        "id": "RiCFkxzWviAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a options\n",
        "df_pyspark_inferschema=spark.read.options(inferSchema=True,header=True).csv('/content/drive/MyDrive/Pyspark/Pyspark_demo.csv')"
      ],
      "metadata": {
        "id": "ACu6oxE7vZjJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use options() it means it gives a multiple arguments but in option() take  single arguments. **Note: inferSchema not work in option().**"
      ],
      "metadata": {
        "id": "UqZsxV_20kmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the informaation of the dataset\n",
        "df_pyspark_inferschema.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Z3Nt0uxfk5",
        "outputId": "2f4b3968-7857-4266-c047-0b30c381765b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Employee name: string (nullable = true)\n",
            " |-- Employee Age : integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Another Way"
      ],
      "metadata": {
        "id": "kp40EHqA1VVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Another way to using inferSchema\n",
        "df_pyspark_inferschema=spark.read.csv('/content/drive/MyDrive/Pyspark/Pyspark_demo.csv',header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "_vJ9QRdfzV5f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the informaation of the dataset\n",
        "df_pyspark_inferschema.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkzgYRs51pAz",
        "outputId": "339c795d-a506-4ea0-c81d-85cd87cd3b95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Employee name: string (nullable = true)\n",
            " |-- Employee Age : integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the columns name in our dataset\n",
        "df_pyspark.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mkgdHSR1rlA",
        "outputId": "3778b677-d310-40c8-9b10-b01b33c00ae2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Employee name', 'Employee Age ', 'Experience', 'Salary']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a perticular colums and see the datatype of that\n",
        "df_pyspark.select('Employee Name')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJ9FSvY1_fG",
        "outputId": "18f7b79a-6c04-4255-ecfa-4a724f31bbec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Employee Name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show a first colums\n",
        "df_pyspark.select('Employee name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inm2X2MF2XQK",
        "outputId": "5b118fe0-a0f4-4ca7-e7ea-1562a61cd1ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|   Employee name|\n",
            "+----------------+\n",
            "|  Mayur Kalamkar|\n",
            "|   Sharad Dhole |\n",
            "|  Rushabh Bhagat|\n",
            "|   Shubham Harne|\n",
            "| Shubham Ambekar|\n",
            "|Ravindra Thombre|\n",
            "|            NULL|\n",
            "|           Akash|\n",
            "|           Pyush|\n",
            "|           Rohan|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now pick a multiple columns\n",
        "df_pyspark.select(['Employee Name', 'Experience']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvvKQBNy2yqL",
        "outputId": "fa744cf7-5b6c-46ce-bfdd-9686dd0ddb77"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------+\n",
            "|   Employee Name|Experience|\n",
            "+----------------+----------+\n",
            "|  Mayur Kalamkar|         8|\n",
            "|   Sharad Dhole |         4|\n",
            "|  Rushabh Bhagat|         3|\n",
            "|   Shubham Harne|        10|\n",
            "| Shubham Ambekar|         1|\n",
            "|Ravindra Thombre|      NULL|\n",
            "|            NULL|      NULL|\n",
            "|           Akash|      NULL|\n",
            "|           Pyush|      NULL|\n",
            "|           Rohan|         5|\n",
            "+----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the datatype of the dataframe\n",
        "df_pyspark.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmc0XQJo3Kw6",
        "outputId": "ef7b9f6d-145d-4d0f-bb2f-361df26930d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Employee name', 'string'),\n",
              " ('Employee Age ', 'string'),\n",
              " ('Experience', 'string'),\n",
              " ('Salary', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding the columns"
      ],
      "metadata": {
        "id": "Udkh0vvfhBHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding column in the datafream\n",
        "df_pyspark.withColumn('Experience After 2 years',df_pyspark['Experience']+2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LaILN3sNUiV",
        "outputId": "08996a15-81d6-4dd3-cccb-7180c41bfcbf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+------------------------+\n",
            "|   Employee name|Employee Age |Experience|Salary|Experience After 2 years|\n",
            "+----------------+-------------+----------+------+------------------------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|                    10.0|\n",
            "|   Sharad Dhole |           26|         4| 40000|                     6.0|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|                     5.0|\n",
            "|   Shubham Harne|           25|        10|100000|                    12.0|\n",
            "| Shubham Ambekar|           28|         1| 10000|                     3.0|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|                    NULL|\n",
            "|            NULL|           23|      NULL| 60000|                    NULL|\n",
            "|           Akash|           21|      NULL| 40000|                    NULL|\n",
            "|           Pyush|         NULL|      NULL|  NULL|                    NULL|\n",
            "|           Rohan|           24|         5|  NULL|                     7.0|\n",
            "+----------------+-------------+----------+------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Droping the columns"
      ],
      "metadata": {
        "id": "C9v46xNuhEEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delate column from datafream\n",
        "df_pyspark.drop('Experience After 2 years').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvmCS21UOV1O",
        "outputId": "470c3de4-4a75-4daa-8a95-096b86261bd0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming  column\n",
        "df_pyspark.withColumnRenamed('Employee Name','Name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_OZS4DUQYIK",
        "outputId": "c82bd739-ec22-4041-8608-a3b9060f3488"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|            Name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Droping Null values"
      ],
      "metadata": {
        "id": "kArSgLpdrzsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see our dataset\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE-Ah0VSsr9S",
        "outputId": "28a7feb8-d950-4c81-ac60-f2c5ed5be368"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop null values from dataset\n",
        "df_pyspark.na.drop().show()"
      ],
      "metadata": {
        "id": "eS4K02B4RD2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06ab738-16e0-4ca5-c39d-616ca57a4d2f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+----------+------+\n",
            "|  Employee name|Employee Age |Experience|Salary|\n",
            "+---------------+-------------+----------+------+\n",
            "| Mayur Kalamkar|           25|         8| 70000|\n",
            "|  Sharad Dhole |           26|         4| 40000|\n",
            "| Rushabh Bhagat|           26|         3| 30000|\n",
            "|  Shubham Harne|           25|        10|100000|\n",
            "|Shubham Ambekar|           28|         1| 10000|\n",
            "+---------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see that all the null rows are remove. last 4 rows are delated. so the drop function drop all the nulll values in the dataset. There are 3 parameters in the drop function i.e. \"How\",\"Thresh\" and \"subset\".  "
      ],
      "metadata": {
        "id": "8eee89Ins5XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Using How"
      ],
      "metadata": {
        "id": "CmK181hbt6H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# any=How\n",
        "df_pyspark.na.drop(how='any').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIKuIfa4sd-c",
        "outputId": "f17f0983-85f1-4bc9-8537-5553d2b674cd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+----------+------+\n",
            "|  Employee name|Employee Age |Experience|Salary|\n",
            "+---------------+-------------+----------+------+\n",
            "| Mayur Kalamkar|           25|         8| 70000|\n",
            "|  Sharad Dhole |           26|         4| 40000|\n",
            "| Rushabh Bhagat|           26|         3| 30000|\n",
            "|  Shubham Harne|           25|        10|100000|\n",
            "|Shubham Ambekar|           28|         1| 10000|\n",
            "+---------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. If \"any\" drop a row if its contain any null values.\n",
        "2. if \"all\" drop only all the null values in the dataset."
      ],
      "metadata": {
        "id": "3o9OVvZauBYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uisng how= all\n",
        "df_pyspark.na.drop(how='all').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TykAVPwHu0gY",
        "outputId": "ea3411ff-cded-4a49-ae04-a1490ba2be51"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically its not work beacuse not all the complate row is null. so it drop all the row which present null values."
      ],
      "metadata": {
        "id": "Zy3wUudyvEBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Using Thresh"
      ],
      "metadata": {
        "id": "44XF5WvHvXU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold\n",
        "df_pyspark.na.drop(how='any', thresh=2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orfJWtuDu8IE",
        "outputId": "b556fd1c-41de-42f3-ec70-d3561ec30ee0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "|           Rohan|           24|         5|  NULL|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically threshold is that is drop the perticalar value in nulll values for example: here threshold =2 so those rows are atleast two null values are presented the row are drop from dataset."
      ],
      "metadata": {
        "id": "uvsPnccBvlbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Using subset"
      ],
      "metadata": {
        "id": "2KwKOu7kwhN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using subset argument in drop method\n",
        "df_pyspark.na.drop(how='any',subset=['salary']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzMzL5iRxgX3",
        "outputId": "faeed901-deaf-491e-b590-a4d0d293a45a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+\n",
            "|   Employee name|Employee Age |Experience|Salary|\n",
            "+----------------+-------------+----------+------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|\n",
            "|   Shubham Harne|           25|        10|100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|\n",
            "|            NULL|           23|      NULL| 60000|\n",
            "|           Akash|           21|      NULL| 40000|\n",
            "+----------------+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a subset is a parameter only remove a null values in particular column.Hrere remove a salary column null values."
      ],
      "metadata": {
        "id": "JIt1WmK2xyJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Filling null values"
      ],
      "metadata": {
        "id": "6WSI6BI5y0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using fillna only argument\n",
        "df_pyspark.fillna(\"Missing Values\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8nFk5DIzAuE",
        "outputId": "541dab02-ea66-446e-e055-946d079ec78c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------+--------------+--------------+\n",
            "|   Employee name| Employee Age |    Experience|        Salary|\n",
            "+----------------+--------------+--------------+--------------+\n",
            "|  Mayur Kalamkar|            25|             8|         70000|\n",
            "|   Sharad Dhole |            26|             4|         40000|\n",
            "|  Rushabh Bhagat|            26|             3|         30000|\n",
            "|   Shubham Harne|            25|            10|        100000|\n",
            "| Shubham Ambekar|            28|             1|         10000|\n",
            "|Ravindra Thombre|Missing Values|Missing Values|         50000|\n",
            "|  Missing Values|            23|Missing Values|         60000|\n",
            "|           Akash|            21|Missing Values|         40000|\n",
            "|           Pyush|Missing Values|Missing Values|Missing Values|\n",
            "|           Rohan|            24|             5|Missing Values|\n",
            "+----------------+--------------+--------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fill or fillna is replace all the null value in given value so, in fill method there are another argument i.e. subset. it basically fill only perticular column values."
      ],
      "metadata": {
        "id": "3LqRZSlxgqit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fillna using subset\n",
        "df_pyspark.fillna(\"Missing Values\",\"Experience\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5npAVSbeQ_J",
        "outputId": "e3bcc9ea-309e-4f1e-e96f-a52360ee4be9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+--------------+------+\n",
            "|   Employee name|Employee Age |    Experience|Salary|\n",
            "+----------------+-------------+--------------+------+\n",
            "|  Mayur Kalamkar|           25|             8| 70000|\n",
            "|   Sharad Dhole |           26|             4| 40000|\n",
            "|  Rushabh Bhagat|           26|             3| 30000|\n",
            "|   Shubham Harne|           25|            10|100000|\n",
            "| Shubham Ambekar|           28|             1| 10000|\n",
            "|Ravindra Thombre|         NULL|Missing Values| 50000|\n",
            "|            NULL|           23|Missing Values| 60000|\n",
            "|           Akash|           21|Missing Values| 40000|\n",
            "|           Pyush|         NULL|Missing Values|  NULL|\n",
            "|           Rohan|           24|             5|  NULL|\n",
            "+----------------+-------------+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Handling Missing Values by mean, median, and mode"
      ],
      "metadata": {
        "id": "0jk4ExXF5_kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Mean"
      ],
      "metadata": {
        "id": "VEcuYH5z6ZWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use immputer feature here\n",
        "from pyspark.ml.feature import Imputer\n",
        "imputer=Imputer(\n",
        "        inputCols=['Employee Age ','Experience','Salary'],\n",
        "        outputCols=[\"{}_imputed\".format(c) for c in ['Employee Age ','Experience','Salary']]\n",
        "        ).setStrategy(\"mean\")"
      ],
      "metadata": {
        "id": "zmxfzZsYhOhg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation estimator for completing missing values, using the mean, median or mode of the columns in which the missing values are located. The input columns should be of numeric type. Currently Imputer does not support categorical features and possibly creates incorrect values for a categorical feature.\n",
        "**Note that the mean/median/mode value is computed after filtering out missing values.** All Null values in the input columns are treated as missing, and so are also imputed. For computing median, pyspark.sql.DataFrame.approxQuantile is used with a relative error of 0.001.\n",
        "\n"
      ],
      "metadata": {
        "id": "0e_TQzR-64iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use trasnform method\n",
        "imputer.fit(df_pyspark_inferschema).transform(df_pyspark_inferschema).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnIwmOkAiEnm",
        "outputId": "e8e1dc84-6336-4e26-a8d5-ba1030bca412"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|   Employee name|Employee Age |Experience|Salary|Employee Age _imputed|Experience_imputed|Salary_imputed|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|                   25|                 8|         70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|                   26|                 4|         40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|                   26|                 3|         30000|\n",
            "|   Shubham Harne|           25|        10|100000|                   25|                10|        100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|                   28|                 1|         10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|                   24|                 5|         50000|\n",
            "|            NULL|           23|      NULL| 60000|                   23|                 5|         60000|\n",
            "|           Akash|           21|      NULL| 40000|                   21|                 5|         40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|                   24|                 5|         50000|\n",
            "|           Rohan|           24|         5|  NULL|                   24|                 5|         50000|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Median"
      ],
      "metadata": {
        "id": "KNjj6op86cbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer=Imputer(\n",
        "      inputCols=['Employee Age ','Experience', 'Salary'],\n",
        "      outputCols=['{}_imputed'.format(c) for c in ['Employee Age ','Experience','Salary']]\n",
        "      ).setStrategy(\"median\")"
      ],
      "metadata": {
        "id": "TdjtaWn2jY18"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(df_pyspark_inferschema).transform(df_pyspark_inferschema).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCjp-Td76ON",
        "outputId": "12b82ed0-4add-4455-db5e-9d96d28dd1ee"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|   Employee name|Employee Age |Experience|Salary|Employee Age _imputed|Experience_imputed|Salary_imputed|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|                   25|                 8|         70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|                   26|                 4|         40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|                   26|                 3|         30000|\n",
            "|   Shubham Harne|           25|        10|100000|                   25|                10|        100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|                   28|                 1|         10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|                   25|                 4|         50000|\n",
            "|            NULL|           23|      NULL| 60000|                   23|                 4|         60000|\n",
            "|           Akash|           21|      NULL| 40000|                   21|                 4|         40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|                   25|                 4|         40000|\n",
            "|           Rohan|           24|         5|  NULL|                   24|                 5|         40000|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Mode"
      ],
      "metadata": {
        "id": "XRE1bPZK8VxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer=Imputer(\n",
        "        inputCols=['Employee Age ','Experience','Salary'],\n",
        "        outputCols=['{}_imputed'.format(c) for c in ['Employee Age ','Experience','Salary']]\n",
        "        ).setStrategy('mode')"
      ],
      "metadata": {
        "id": "YgLGDT5W8Qo3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(df_pyspark_inferschema).transform(df_pyspark_inferschema).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obxBz46F8-GE",
        "outputId": "5bf8d138-d2a9-4dc6-9432-4ba5398c6bd7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|   Employee name|Employee Age |Experience|Salary|Employee Age _imputed|Experience_imputed|Salary_imputed|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "|  Mayur Kalamkar|           25|         8| 70000|                   25|                 8|         70000|\n",
            "|   Sharad Dhole |           26|         4| 40000|                   26|                 4|         40000|\n",
            "|  Rushabh Bhagat|           26|         3| 30000|                   26|                 3|         30000|\n",
            "|   Shubham Harne|           25|        10|100000|                   25|                10|        100000|\n",
            "| Shubham Ambekar|           28|         1| 10000|                   28|                 1|         10000|\n",
            "|Ravindra Thombre|         NULL|      NULL| 50000|                   25|                 1|         50000|\n",
            "|            NULL|           23|      NULL| 60000|                   23|                 1|         60000|\n",
            "|           Akash|           21|      NULL| 40000|                   21|                 1|         40000|\n",
            "|           Pyush|         NULL|      NULL|  NULL|                   25|                 1|         40000|\n",
            "|           Rohan|           24|         5|  NULL|                   24|                 5|         40000|\n",
            "+----------------+-------------+----------+------+---------------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XN0hXuSC9Jeq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}